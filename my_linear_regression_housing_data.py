# -*- coding: utf-8 -*-
"""My Linear Regression- Housing_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KudaZb5frzYNGN-TWSQXXmgCDCvLas1l

# Linear Regression in Python from Scratch

We are going to implement linear regression from scratch.
"""

# Commented out IPython magic to ensure Python compatibility.
#importing dependencies

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

#Reading data from the CSV file
datafile = 'Housing_data.txt'
#Read into the data file
cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2,3),unpack=True) #Read in comma separated data
#Usecols() will give how many columns to read
#Form the usual "X" matrix and "y" vector
X = np.transpose(np.array(cols[:-1]))
#print(X)
y = np.transpose(np.array(cols[-1:]))
#print(y)
m = y.size # number of training examples
#Insert the usual column of 1's into the "X" matrix
X = np.insert(X,0,1,axis=1)
#numpy.insert(arr, obj, values, axis) will broadcast 1 to all rows and will make a column

print(X.shape)
print(y.shape)

print(np.mean(X[:,1]))
print(np.std(X[:,1]))

#Here we have to do normalization on the data as it has values of different ranges.
stored_feature_means, stored_feature_stds = [], []
Xnorm = X
for col in range(Xnorm.shape[1]):
    if(col !=0):
        stored_feature_means.append(np.mean(Xnorm[:,col]))
        stored_feature_stds.append(np.std(Xnorm[:,col]))
        #Skip the first column
        #Faster to not recompute the mean and std again, just used stored values
        Xnorm[:,col] = (Xnorm[:,col] - stored_feature_means[-1])/stored_feature_stds[-1]

print(np.mean(Xnorm[:,1]))
print(np.std(Xnorm[:,1]))
print(stored_feature_means)
print(stored_feature_stds)

#after feature normalization we move on to the main algorithm

#linear function hypothesis
def h(X,theta): #Linear hypothesis function
    return np.dot(X,theta) #h(x)=theta*X ...both are vectors

"""
theta_start is an n- dimensional vector of initial theta guess
X is matrix with n- columns and m- rows
y is a matrix with m- rows and 1 column
"""

#Compute Cost
def compute_cost(theta,X,y):
    pred=h(X,theta) #calling the hypothesis for predictions
    
    sqr_error=(pred-y)**2 #Calculating the squared errors
    J=1/(2*m)*sum(sqr_error) # Cost function - totaling the sqr_error and dividing it with total instances
    return (J[0])

#Intializing theta value as a zero/null matrix
initial_theta=np.zeros((X.shape[1],1)) #means it has rows=no. of coloums of X and 1 coloumn

print("The initial cost with Theta initialized as zero:")
print(compute_cost(initial_theta,X,y))



errors=h(X,initial_theta) - y
print(initial_theta.shape)
print(errors.shape)
print(X.shape)

#Actual Gradient Decent Algorithm for minimizing cost
def Gradient_decend(X,y,theta,num_iters,alpha):
    m=y.shape[0]
    J_hist=np.zeros((num_iters,1))
    #J_hist for ploting for looking at the Error
    for iter in range(0,num_iters):
        errors=h(X,theta) - y
        Xnew=np.dot(X.T,errors)
        #Calculating the new theta
        theta_change=(alpha*(1/m)) * (Xnew)
        theta= theta-theta_change
        #Updating simultaneously theta every iteration
        
        J_hist[iter]=compute_cost(theta,X,y)
            
    return (theta,J_hist)

print("Initial values of theta")
print(initial_theta)
print("With dimentions : "+str(initial_theta.shape))
#print(theta_change.shape)

#Initialization
iterations=1500
alpha=0.01

#Run gradient descent with multiple variables, initial theta still set to zeros
#(Note! This doesn't work unless we feature normalize! "overflow encountered in multiply")
initial_theta = np.zeros((Xnorm.shape[1],1))
theta, j_vec = Gradient_decend(Xnorm,y,initial_theta,iterations,alpha)
print("The updates values of theta are :")
print(theta)

#Plot the convergence of the cost function
def plotConvergence(jvec):
    plt.figure(figsize=(10,6))
    plt.plot(range(len(jvec)),jvec,'bo')
    plt.grid(True)
    plt.title("Convergence of Cost Function")
    plt.xlabel("Iteration number")
    plt.ylabel("Cost function")
    dummy = plt.xlim([-0.05*iterations,1.05*iterations])
    #dummy = plt.ylim([4,8])

#Plot convergence of cost function:
plotConvergence(j_vec)

#print "Final result theta parameters: \n",theta
def predict(ytest):    
    print ("Check of result: What is price of house with "+ str(ytest[0]) +" square feet and "+str(ytest[1])+ " bedrooms and with an age of "+str(ytest[2])+ " years?")
    #To "undo" feature normalization, we "undo" 1650 and 3, then plug it into our hypothesis
    ytestscaled = [(ytest[x]-stored_feature_means[x])/stored_feature_stds[x] for x in range(len(ytest))]
    #print(ytestscaled)
    ytestscaled.insert(0,1)
    print ("$ %0.2f" % float(h(ytestscaled,theta)))

ytest=np.array([1203,3,11])
predict(ytest)

